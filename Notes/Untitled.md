### 1.

After watching the video, the most impressive thing to me was Shannon’s 1937 thesis. It’s wild to think that he just looked at a bunch of messy electrical relays and realized they were essentially doing Boolean algebra. Before him, people just saw switches as hardware, but he formalized them with mathematics. 

Another thing that stood out was how he decided to separate meanings from messages . Usually, we think of information as "meaning," but Shannon realized that for a mathematical theory to work, you have to decouple the two. He focused on the choice between possibilities rather than the content itself. It demonstrates how Shannon used to think about things in an abstract way. 

The origin of the "Bit" is also interesting. You don’t usually think about it, but by formalizing binary as the standard, he basically gave the tech world a common tongue. It took all these isolated inventions and finally forced them to play nice together to initiate the information age.

### 2. 

As a physics student, I’m curious about the connection between Information Entropy and Thermodynamic Entropy. The video mentions that Von Neumann told Shannon to use the word "entropy" because the math looked the same, but it feels a bit like a "teaser." Is this just a formal coincidence in the formulas, or is there a deeper physical link between the microstates of a gas and the bits in a message? 

Second, the Noisy Channel Coding Theorem sounds almost impossible. The idea that you can send data with zero error through a noisy line, as long as you stay under a certain limit, is counterintuitive. I get the basic idea of adding "extra bits" for redundancy, but how do you add enough redundancy to fix errors without making the whole transmission too slow?

Third, I’d like to hear more about the engineering skepticism with digital circuits. The 1940s were all about analog signals and continuous waves. Moving to discrete digital must have seemed bold. What were the actual physical hardware hurdles they faced at Bell Labs when trying to prove that digital was actually more robust than analog?

### 3. 

We’re getting closer and closer to the "Shannon Limit" with our current fiber and silicon tech, so is the next big leap going to require a Quantum Information Theory that replaces the classic bit with qubits to fundamentally break through these classical communication limits?