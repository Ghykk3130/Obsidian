In order for an LLM to generate reasonable text, we need the attention mechanism. Without attention, an LLM cannot capture long-range dependencies, and text that requires such dependencies may fail to be generated correctly. For example, in the sentence: “The cat left its litter mates and uses the litter x,” where x is the word we want to generate, we expect a strong association between “cat” and _x_. Without attention to capture this relationship, the model might only consider the local context “litter” and generate a less appropriate word.

Attention is a mechanism for computing the relationships between tokens. It is called “attention” because humans allocate focus when receiving information, attending to the most relevant details at each moment. Similarly, the attention mechanism in an LLM assigns different weights to words depending on their relevance. Given a text, a tokenizer converts it into a sequence of tokens. After embedding, we obtain a vector representing each token. Attention then computes the similarity between every pair of tokens (for example, using cosine similarity). The transformer merges two similar tokens in some way, producing a new token that encodes information from both. Repeating this process of embedding and attention produces a series of abstract tokens that contain information from the original text. The decoder then assigns scores to possible next tokens based on these abstract tokens, and sampling generates the most likely next word.

However, even with attention, LLMs do not always generate correct information. The phenomenon of producing incorrect outputs is called the hallucination problem. LLMs do not “know” facts in a human sense; they generate the most likely next token based on patterns learned from training data. Attention ensures better generation quality but cannot guarantee correctness. Fundamentally, an LLM is a probabilistic model predicting the next token. It can generate the most probable token, but it may not be true. If a fact is not clearly represented in training, the model may fabricate something to fill the gap. The implication of hallucination is that truthfulness cannot be guaranteed. LLMs may produce answers that users want to hear, but there is no mechanism ensuring correctness. Therefore, users must maintain critical oversight.

The hallucination problem can be mitigated using RAG and CoT. RAG allows the LLM to retrieve information from external sources (like the Internet) and incorporate it into the prompt before generation. Because the prompt contains real information, the model is more likely to produce accurate outputs. CoT encourages the LLM to mimic human problem-solving by breaking a question into subproblems and reasoning step by step. Previously, the model might try to generate the final answer directly; CoT forces stepwise verification, reducing blind guessing.

Both RAG and CoT resemble human memory and reasoning. RAG allows the model to retrieve relevant information from external sources, analogous to humans searching their “mental library” for relevant knowledge. CoT is even closer to human reasoning: it mirrors how humans break down problems, check intermediate steps, and use logical reasoning, combining understanding and experience to reach conclusions.