Tokenization是一种将文本分成板块的过程。给定一段文本，tokenizer会根据一定的规则将文本分成板块。每个板块称为一个token。每个token会被给予一个unique id和embedding，以便将文本转化为机器能够解读的形式。LLM在接受这些token后，能根据自己的训练产生合适的，接下来的token。因此token是LLM理解文本的最小结构。与其说LLM是接受单词，预测单词的黑箱，不如说LLM实际上是接受token，预测token的黑箱。

一般来说，存在三种tokenization的方式。第一种tokenization的方式是word-based tokenization。给定一段文本，word-based tokenizer将文本中每个单词作为一个token。第二种tokenization的方式是character-based tokenization。给定一段文本，character-based tokenizer将文本中每个character作为一个token。第三种tokenization的方式是subword tokenization。给定一段文本，subword tokenizer会将最常出现的字符组作为token。这里一个字符组不一定是一个单词。

Modern LLMs tend to use subword tokenization instead of word or character-based tokenization. 这是因为subword tokenization相比于后两种tokenization具有一定优势。subword tokenization比起word-based tokenization来说，lexical coverage更小。这意味着基于subword tokenization的LLM比基于word-based tokenization的LLM具有更快的运行速度与更高的灵活性。这是因为它所需要处理的token比word-based tokenizer需要处理的更少。比起character-based tokenization，subword tokenization具有更高的level of abstraction。因此使用subword tokenization的LLM比起使用character-based tokenization的LLM更善于language modeling任务。

Subword tokenization和morphology in natural language是相似而又不同的概念。具体来说，morphology可以作为一种subword tokenization的标准。这是因为morphology将文本按照人类自然语言的方式拆解为词根词缀。这些词根词缀符合token的定义，可以作为subword token。但反过来说，subword token不一定是morpheme。因为subword token仅仅是将出现频率高的字符作为token，但出现频率高的字符可能没有morphological meaning。例如vanquish中的qui可能是一个token，但qui没有任何意义。

造成LLM难以准确识别一个字符中某个字母的个数的原因是tokenization。原则上讲，token的unique id中并不显含token对应的文本中单词拼写的信息。例如在LLM看来，the word strawberry is a sequence of abstract ids and embeddings. It doesn't inherently know the word contains the letter r. 类似的，在遇到数字时，tokenizer会将数字分开。例如9.9可能是一个token，而9.11可能被拆为两个token。因此LLM只能看到它们的token而不是numerical values，进而不能从数学上比较它们的大小。

我们可以推测，由于tokenization，LLM不能从给定化学物质的化学式推断出化学名称。因为化学式被tokenize成了更小的部分。这每个更小的部分都没有任何化学意义。LLM也不能完成破解密码的任务。因为密码本身完全和人类语言不通。LLM按照人类语言中token出现频率来tokenize一段密码是读不出任何意义的。

然而，将tokenization的过程细致化可以部分解决naive tokenization带来的问题。在naive tokenization中，tokenization的规则基于人类语言。人类语言中出现频率越高的字符组合就越有可能成为token。通过针对不同类型的文本改变tokenization的规则，就可以使LLM正确地tokenize除人类语言之外的文本。例如说，在遇到数学算式时，tokenizer如果按照数学公式中字符出现的频率进行tokenization，那么就很有可能正确地解读数学算式。同样地，在解读化学式时，tokenizer若按照化学式中字符出现的频率进行tokenization，那么也有很大可能正确地理解化学式。

但是通过细化tokenization不能解决全部问题。
