Tokenization是一种将文本分成板块的过程。给定一段文本，tokenizer会根据一定的规则将文本分成板块。每个板块称为一个token。每个token会被给予一个unique id和embedding，以便将文本转化为机器能够解读的形式。LLM在接受这些token后，能根据自己的训练产生合适的，接下来的token。因此token是LLM理解文本的最小结构。与其说LLM是接受单词，预测单词的黑箱，不如说LLM实际上是接受token，预测token的黑箱。

一般来说，存在三种tokenization的方式。第一种tokenization的方式是word-based tokenization。给定一段文本，word-based tokenizer将文本中每个单词作为一个token。第二种tokenization的方式是character-based tokenization。给定一段文本，character-based tokenizer将文本中每个character作为一个token。第三种tokenization的方式是subword tokenization。给定一段文本，subword tokenizer会将最常出现的字符组作为token。这里一个字符组不一定是一个单词。

Modern LLMs tend to use subword tokenization instead of word or character-based tokenization. subword tokenization比起word-based tokenization来说，lexical coverage更小。这意味着基于subword tokenization的LLM比基于word-based tokenization的LLM具有更快的运行速度。这是因为它所需要处理的token比word-based tokenizer需要处理的更少。比起character-based tokenization，